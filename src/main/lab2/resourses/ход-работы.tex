\subsection{Аналитическое решение}\label{subsec:аналитическое-решение}
Аналитически найдём минимум функции {$f(x,y) = 4x^2 + 6xy + 4y^2 + 8x + 10y + 1$}.
\newline
C помощью WolframAlpha получаем точку минимума \{$-1/7;-8/7$\}.

\subsection{Общая схема поиска}\label{subsec:общая-схема-поиска}
Будем строить последовательность точек {$x^k$}, сходящихся к точке минимума $x$. \newline
Для этого на каждом шаге будем находить \newline
{$x^{k+1}=x^k+\alpha^{k}*p^k$}, где {$p^k$} - вектор направления убывания, {$\alpha^k$} > 0, \newline
{$f(x^k+\alpha^k*p^k) < f(x^k)$}.\newline
Критерием окончания поиска считаем: ||\nabla {f(x^k)}||< {$\varepsilon$}.
\newline

\subsection{Метод градиентного спуска}\label{subsec:метод-градиентного-спуска2}
Метод градиентного спуска заключается в том, что на каждом шаге мы пытаемся пойти по направлению наискорейшего убывания функции(антиградиенту).
Идем с шагом $\lambda$, уменьшаем его в два раза, если не попали в меньшую точку. \newline
Рассмотрим метод работы градиентного спуска на примере {$f(x,y) = 4x^2 + 6xy + 4у^2 + 8x + 10у + 1$}.
Начальное приближение: точка {$x = (1, -1)$}. \newline
Начальное значение $\lambda$ = 0.01. \newline
Число итераций: $?$. \newline
Конечный модуль градиента: $?$.\newline
Результат: $Xmin = (-0.13938510361806328; -1.1463291820962223)$, $Fmin = ?$.
\newline
\newline
Таблица, отражающая скорость сходимости, и график сходимости:

\subsubsection{Исследование зависимости числа итераций от числа обусловленности оптимизируемой функции}
\newline
\newline

\subsubsection{Вывод}
В градиентном методе с дроблением шага параметр $\lambda$ выбирается изначально и в последующих итерациях делится на два, что позволяет быстро пересчитывать $\lambda$ на каждом шаге.
Для более точного результата начальное значение $\lambda$ должно равняться {$2/(l + L)$}, где $l$ и $L$  - наибольшое и наименьшее собственное число матрицы $A$.
Однако, нахождение собственных значений произвольной матрицы - задача нетривиальная и ресурсозатратная, поэтому мы фиксируем $\lambda = 0.01$.
\newline
\newline
Как видно из таблицы, вблизи $Xmin$ скорость сходимости заметно убывает.
Это связано с тем, что модуль градиента становится малой величиной, засчет чего график сходимости имеет зигзагообразный характер.
Одним из вариантов решения этой проблемы является нормализация градиента, однако это требует дополнительных вычислений.
\newline
\newline
Обычно вычисление градиента является более ресурсозатратной операцией, чем вычисление значения функции, поэтому метод градиентного спуска не является оптимальным.
Число итераций зависит линейно от числа обусловленности.
\newpage

\subsection{Метод наискорейшего спуска}\label{subsec:метод-наискорейшего-спуска2}
Метод наименьшего спуска заключается в том, что на каждом шаге пересчитывается градиент функции, пока функция убывает и выполняется поиск $\alpha$ с помощью одномерной оптимизации:
\newline
$F^k = f(x^k + \alpha \cdot \nabla{f(x^k)}$
\newline
Рассмотрим метод работы наискорейшего спуска на примере {$f(x,y) = 4x^2 + 6xy + 4у^2 + 8x + 10у + 1$}.
Начальное приближение: точка {$x = (1, -1)$}. \newline
Начальное значение $\lambda$ = 0.01. \newline
Число итераций: $?$. \newline
Конечный модуль градиента: $?$.\newline
Результат: $Xmin = (-0.13938510361806328; -1.1463291820962223)$, $Fmin = ?$.
\newline
\newline
Таблица, отражающая скорость сходимости, и график сходимости:

\subsubsection{Исследование зависимости числа итераций от числа обусловленности оптимизируемой функции}
\newline
\newline

\subsubsection{Вывод}
Использование одномерной оптимизации дает преимущество в скорости перед методом градиентного спуска, но также делает пересчет каждого шага более ресурсозатратным.
Сходимость зигзагообразная, скорость сходимости увеличилась относительно метода градиентного спуска(?).
Число итераций итераций уменьшилось, потому что мы не уменьшаем шаг.
Оно зависит от числа обусловленности, но не от размерности рассматриваемой функции.

\newpage

\subsection{Метод сопряженных градиентов}\label{subsec:метод-сопряженных-градиентов2}
В этом методе $p^k$ выбираются $A$-ортогональными, и такой последовательный спуск приведет к точке минимума квадратичной функции не более, чем за {размерность матрицы} шагов.
Кроме того, за счет использования раннее вычисленных значений $p^k$, более полно учитываются особенности функции.
\newline
$p^(k+1) = -\nabla{f(x^k + 1)} + \beta^k \cdot p^k$
\newline
Рассмотрим метод работы градиентного спуска на примере {$f(x,y) = 4x^2 + 6xy + 4у^2 + 8x + 10у + 1$}.
Начальное приближение: точка {$x = (1, -1)$}. \newline
Начальное значение $\lambda$ = 0.01. \newline
Число итераций: $?$. \newline
Конечный модуль градиента: $?$.\newline
Результат: $Xmin = (-0.13938510361806328; -1.1463291820962223)$, $Fmin = ?$.
\newline
\newline
Таблица, отражающая скорость сходимости, и график сходимости:

\subsubsection{Исследование зависимости числа итераций от числа обусловленности оптимизируемой функции}
\newline
\newline

\subsubsection{Вывод}
Метод сопряженных градиентов сходится к решению не более, чем за {размерность матрицы} итераций, но т.к. погрешности при вычислениях накапливаются, вектора могут получаться не ортогональными и $\beta^k$ надо сбрасывать каждые {размерность матрицы} итераций.
Для неквадратичной функции не гарантируется сходимость за {размерность матрицы} шагов, но если она аппроксимируется квадратичной функцией, метод гарантирует быструю сходимость.
Метод решает задачу минимализации квардратичных функций за конечное число шагов, но на общей функции может попасть в область локального минимума и не найти глобальный минимум.
\newpage

\subsection{Анализ траекторий}\label{subsec:анализ-траекторий}
\newpage


\section{Вывод}\label{sec:вывод}
\newline
Самым быстрым оказался метод сопряженных градиентов, его рост числа итераций от числа обусловленности логарифмический, тогда как у метода градиентного спуска и метода наискорейшего спуска линейная зависимость.
